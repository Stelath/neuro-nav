{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from bci_dataset import BCIDataset\n",
    "\n",
    "from brainflow.board_shim import BoardShim, BrainFlowInputParams, BoardIds\n",
    "from brainflow.data_filter import DataFilter, FilterTypes, WindowOperations, NoiseTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCIModel(nn.Module):\n",
    "    def __init__(self, classes=3):\n",
    "        super(BCIModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(8, 64, 1)\n",
    "        self.conv2 = nn.Conv1d(64, 256, 1)\n",
    "        self.conv3 = nn.Conv1d(256, 512, 1)\n",
    "        self.conv4 = nn.Conv1d(512, 1024, 1)\n",
    "        self.conv5 = nn.Conv1d(1024, 16, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16*64, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        x = torch.sigmoid(self.conv3(x))\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        x = torch.sigmoid(self.conv5(x))\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stelath/GitHub/neuro-nav/bci_dataset.py:40: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  inp[k] = DataFilter.perform_fft(channel[: hz // 2], WindowOperations.NO_WINDOW.value)[:hz // 4]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BCIDataset('raw_data_5min.npy')\n",
    "test_dataset = BCIDataset('raw_data_test.npy')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2048, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BCIModel()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_pass(inputs, targets, train=False):\n",
    "  inputs = inputs.to(device)\n",
    "  targets = targets.to(device)\n",
    "\n",
    "  if train:\n",
    "    model.zero_grad()\n",
    "  \n",
    "  outputs = model(inputs)\n",
    "  matches = [torch.argmax(i) == torch.argmax(j) for i, j in zip(outputs, targets)]\n",
    "  acc = matches.count(True) / len(matches)\n",
    "  loss = loss_function(outputs, targets)\n",
    "\n",
    "  if train:\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "  return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad0ad7c0635466d9d2454ecec33877f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m inputs \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m targets \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m acc, loss \u001b[39m=\u001b[39m fwd_pass(inputs, targets, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     16\u001b[0m loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     18\u001b[0m progress_bar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m, in \u001b[0;36mfwd_pass\u001b[0;34m(inputs, targets, train)\u001b[0m\n\u001b[1;32m      9\u001b[0m matches \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39margmax(i) \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39margmax(j) \u001b[39mfor\u001b[39;00m i, j \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(outputs, targets)]\n\u001b[1;32m     10\u001b[0m acc \u001b[39m=\u001b[39m matches\u001b[39m.\u001b[39mcount(\u001b[39mTrue\u001b[39;00m) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(matches)\n\u001b[0;32m---> 11\u001b[0m loss \u001b[39m=\u001b[39m loss_function(outputs, targets)\n\u001b[1;32m     13\u001b[0m \u001b[39mif\u001b[39;00m train:\n\u001b[1;32m     14\u001b[0m   loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/neuronav/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/neuronav/lib/python3.8/site-packages/torch/nn/modules/loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/miniconda3/envs/neuronav/lib/python3.8/site-packages/torch/nn/functional.py:3095\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3092\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n\u001b[1;32m   3093\u001b[0m     weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3095\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight, reduction_enum)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d:%m:%Y-%H:%M:%S\")\n",
    "writer = SummaryWriter(f'runs/BCI_{dt_string}')\n",
    "\n",
    "EPOCHS = 60\n",
    "global_step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    progress_bar = tqdm(total=len(train_dataloader))\n",
    "    progress_bar.set_description(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        inputs = batch['inputs']\n",
    "        targets = batch['targets']\n",
    "        \n",
    "        acc, loss = fwd_pass(inputs, targets, train=True)\n",
    "        loss = loss.detach().item()\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        logs = {\"loss\": loss, \"accuracy\": acc, \"step\": step}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        \n",
    "        writer.add_scalar('Loss/train', loss, global_step)\n",
    "        writer.add_scalar('Accuracy/train', acc, global_step)\n",
    "        \n",
    "        \n",
    "        global_step += 1\n",
    "        \n",
    "    \n",
    "    test_acc_list = []\n",
    "    test_loss_list = []\n",
    "    for batch in test_dataloader:\n",
    "        inputs = batch['inputs']\n",
    "        targets = batch['targets']\n",
    "        \n",
    "        test_acc, test_loss = fwd_pass(inputs, targets, train=False)\n",
    "        \n",
    "        test_acc_list.append(test_acc)\n",
    "        test_loss_list.append(test_loss.detach().item())\n",
    "    \n",
    "    test_acc = np.mean(test_acc_list)\n",
    "    test_loss = np.mean(test_loss_list)\n",
    "    \n",
    "    writer.add_scalar('Loss/test', test_loss, global_step)\n",
    "    writer.add_scalar('Accuracy/test', test_acc, global_step)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch + 1 == EPOCHS:\n",
    "        torch.save(model.state_dict(), f\"models/model_{epoch + 1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuronav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d5aef1c97cce0d8b1b8135a3f65fe22a8049f381bd703d7f19d42d052a45954"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
